#!/usr/bin/python3
"""Process data generated by hogwild runs"""
# pylint: disable=C0103

from multiprocessing import Pool
import argparse
import logging
import os
import time
from itertools import count
from functools import partial
import matplotlib.pyplot as plt
import numpy as np

parser = argparse.ArgumentParser(description='Wrapper for data parallelism')

NUM_WORKERS = 10
NUM_POINTS = -50*1000


def load_csv_file(fname, skip_header=0, skip_size=1):
    """Generic function to load a formatted csv file

    Very useful to parallelize loading. Returns None on failure, to allow for
    pruning of failed files without crashing

    Looks through the loaded values, and offsets as necessary - useful when
    runs were made in pieces (eg, manual learning rate decay or simulated
    attack)"""
    if os.path.isfile(fname):
        # pylint: disable=E1101
        data = np.genfromtxt(fname, delimiter=',', dtype=float,
                             skip_header=skip_header)

        # handle appended logs by offsetting each appended time by the end time
        # of the previous log -> converts all time into a monotonically
        # increasing counter
        accum_total = 0
        for i in range(0, len(data) - 1, skip_size):
            if i > 0 and data[i, 0] < data[i-1, 0]:
                data[i:, 0] += data[i-1, 0] - accum_total
                accum_total = data[i-1, 0]
                logging.info('Found an appended log for %s at %i', fname, i)

        return data
    else:
        logging.error("%s not found", fname)
        return None


class hogwild_run(object):
    """Object to hold information for each run, for post-processing

    If initiated with a filepath, it will extract the run configuration from
    the file name.

    This object will also generate filenames (optionally, with filepaths) from
    the given configuration for a user-specified number of runs

    File formats should be:
        Single run -> [name]-[workers], eg baseline-3, indsc-3
        Multiple runs -> [name]-[workers]-[run], eg baseline-3-0, indsc-3-0
        targeted -> [name]-[workers]-[target]-[bias]-[run], eg targ-3-6-20-0
    """

    def __init__(self, filepath=None):
        """Either instantiate an empty object or extract information from path

        If empty (ie, no path), must call setup before any further use"""
        if filepath is None:
            logging.debug('Instantiated an empty run')
            self.runname = None
            self.workers = None
            self.target = None
            self.bias = None
            self.single_run = None
            self.path = None
            self.runs = None

        # extract the run info from the file name
        # SETS UP PATH
        else:
            logging.debug('Instantiated a run from a filepath %s', filepath)
            # remove the file extension
            filepath = filepath[:-1] if filepath.endswith('/') else filepath
            runname = filepath.split('/')[-1].split('.')[0]
            runname = runname.split('-')  # break into individual components
            filepath = '/'.join(filepath.split('/')[:-1])

            if len(runname) == 2:  # single run
                self.setup(runname[0], workers=runname[1], single_run=True,
                           path=filepath)

            elif len(runname) == 3:  # non-targeted multiple run
                self.setup(runname[0], workers=runname[1], single_run=False,
                           path=filepath)

            elif len(runname) == 5:  # targeted run (multiple runs)
                self.setup(runname[0], workers=runname[1], target=runname[2],
                           bias=runname[3], path=filepath)
            else:
                raise NotImplementedError

    def setup(self, runname, workers=1, target=None,  # pylint: disable=R0913
              bias=None, single_run=False, path=None, runs=5):
        """Assign run configuration information

        Called by init, or allows a manual user override. This function must be
        called at least once before using any others"""
        self.runname = runname
        self.workers = workers
        self.target = int(target)
        self.bias = bias
        self.single_run = single_run
        self.path = path
        self.runs = runs
        logging.debug('runname is %s', runname)
        logging.debug('workers is %s', workers)
        logging.debug('target is %s', target)
        logging.debug('bias is %s', bias)
        logging.debug('single_run is %s', single_run)
        logging.debug('path is %s', path)
        logging.debug('runs is %s', runs)

    def format_name(self, append=None):
        """Generate filename strings to match hogwild runs

        Do not use this information directly unless there is a single run and
        no run information in the filename! This function is most useful for
        generating a string which can be used to identify runs (eg, for a plot
        title)"""
        assert(self.runname is not None), 'get_filename called before setup!'

        if self.target is not None and self.bias is not None:
            rval = "{}-{}-{}-{}".format(self.runname, self.workers,
                                        self.target, self.bias)

        else:
            assert(self.target is None), 'Target is set but bias is not!'
            assert(self.bias is None), 'Bias is set but target is not!'
            rval = "{}-{}".format(self.runname, self.workers)

        if append is None:
            return rval
        else:
            return "{} - {}".format(rval, append)

    def get_filename(self, runs=None):
        """Same as format_name, but returns a list of filenames instead

        This function does return run information too, hence the list. This is
        the function which should be used to actually access files"""
        assert(self.runname is not None), 'get_filename called before setup!'
        runs = self.runs if runs is None else runs

        if self.single_run:
            # only ran a single instance of each single_run, doesn't have
            # multiple runs!
            return [self.format_name()]

        else:
            return ["{}-{}".format(self.format_name(), run) for run in
                    range(runs)]

    def get_fullnames(self, path=None):
        """Use this function to load data files

        Uses the output of get_filename (because it inclues the run
        information), and prepends a path.

        Will not return a filepath unless it can verify it's existence"""
        npath = path if path is not None else self.path
        assert(npath is not None), 'Path was not assigned'

        names = ["{}/{}.hogwild".format(npath, fname) for fname in
                 self.get_filename()]
        logging.debug('Before pruning: %s', names)
        names = [x for x in names if os.path.exists(x)]
        logging.debug('After pruning: %s', names)
        assert(len(names) != 0), 'No folder matching this configuration found!'

        return names

    def find_runs(self):
        """Look for runs, instead of forcing the user to specify"""
        # TODO look for files with matching file names and different run
        # numbers to count the runs
        raise NotImplementedError

    def load_all_preds(self):
        """load confidence files for each run -> single run at a time"""
        load_func = partial(load_csv_file, skip_header=0)
        loaded_preds = []
        for run in self.get_fullnames():
            logging.info('Loading run %i', run)
            with Pool(NUM_WORKERS) as p:  # pylint: disable=E1129
                data = p.map(load_func, ["{}/conf.{}".format(run, corr_label)
                                         for corr_label in range(10)])

            # make sure all predictions loaded correctly
            # Process them in a separate loop to avoid any wasted work, ie, the
            # predictions for the last label failed to load but the following
            # loop would process the first 9 before failing and discarding the
            # work!
            append = True
            for idx, preds in enumerate(data):
                if preds is None:
                    append = False
                    logging.error('Failed to load predictions for %s in %s',
                                  idx, run)

            if append:
                loaded_preds.append(data)  # add the current run to the list

        assert(len(loaded_preds) != 0), 'No predictions loaded correctly'

        return loaded_preds

    def load_all_eval(self):
        """Load all eval files

        Uses get_fullname, so a path must be set before using this function

        Parallelizes across evaluation files... This really isn't necessecary
        when multiple runs aren't used, but it's helpful when eval files are
        large AND multiple runs are used"""
        func = partial(load_csv_file, skip_header=1)
        with Pool(NUM_WORKERS) as p:  # pylint: disable=E1129
            data = p.map(func, ["{}/eval".format(fname) for fname in
                                self.get_fullnames()])
        return [x for x in data if x is not None]


def average_at_evals(curr_run, pred_rate=None):
    """Average the confidences for each evaluation - ie, all the ones with the
    same time

    Call once for each run
    """
    mean_func = partial(np.mean, axis=0)
    fdata = []  # list of all labels for the current run
    fpdata = []
    for corr_label, curr_pred in zip(curr_run, pred_rate if pred_rate is not
                                     None else count()):
        # pylint: disable=E1101

        # Truncate partial predictions (eg, if a run was stopped early and eval
        # did not complete)
        sdata = []  # data, separated by step
        data_step = []  # a single step
        pdata = []  # prediction rates, separated by step

        curr_time = corr_label[0, 0]
        start_idx = 0
        for end_idx, c_row in enumerate(corr_label):
            if c_row[0] != curr_time:
                sdata.append(data_step)
                data_step = []
                curr_time = c_row[0]
                if pred_rate is not None:
                    labels, counts = np.unique(curr_pred[start_idx:end_idx],
                                               return_counts=True)
                    rates = np.zeros(10)
                    for idx, lbl in enumerate(labels):
                        rates[int(lbl)] = counts[idx]
                    pdata.append(rates)
                    start_idx = end_idx
            data_step.append(c_row)
        sdata.append(data_step)
        if pred_rate is not None:
            labels, counts = np.unique(curr_pred[start_idx:end_idx],
                                       return_counts=True)
            rates = np.zeros(10)
            for idx, lbl in enumerate(labels):
                rates[int(lbl)] = counts[idx]
            pdata.append(rates)

        # Find the average confidence for each class over all
        # images belonging to that class
        #
        # limit to 10 threads to make condor scheduling
        # deterministic
        with Pool(NUM_WORKERS) as p:  # pylint: disable=E1129
            sdata = p.map(mean_func, sdata)

        fdata.append(sdata)
        fpdata.append(pdata)

    if pred_rate is None:
        return fdata
    else:
        return fdata, fpdata


def compute_targeted(curr_run, runInfo):
    """Find the tolerance of the correct label to the target label for the
    run"""
    # assert(runInfo.target is not None), 'Target cannot be none'

    tolerance_to_targ = []
    # pylint: disable=E1101
    strt = time.process_time()
    for cidx, corr_label in enumerate(curr_run):
        tol = np.zeros((len(corr_label), 2))
        tol[NUM_POINTS:, 0] = corr_label[NUM_POINTS:, 0]
        tol[NUM_POINTS:, 1] = corr_label[NUM_POINTS:, cidx+1] - \
            corr_label[NUM_POINTS:, runInfo.target]
        tolerance_to_targ.append(tol[NUM_POINTS:])
    logging.info('%.4fS to compute', time.process_time() - strt)

    return average_at_evals(tolerance_to_targ)


def subtract_max(row, corr):
    """Compute the difference between the correct prediction and the next
    highest confidence prediction

    Computes only on a single row - this function is split over all rows by a
    mp pool"""
    # nrow = np.zeros(2)
    # nrow[0] = row[0]  # keep timing information

    # Only take the max over non-correct predictions
    candidates = np.zeros(9)
    candidates[:corr] = row[1:corr+1]  # all labels before correct
    candidates[corr:] = row[corr+2:]  # all labels after correct
    # nrow[1:] = row[corr+1] - np.max(candidates)

    return row[0], row[corr+1] - np.max(candidates), np.argmax(row[1:])
    # return [nrow, np.argmax(row)]


def compute_indiscriminate(curr_run):
    """Find the tolerance of the correct label to the next highest confidence
    prediction"""
    tolerance_to_any = []
    prediction_rates = []
    # pylint: disable=E1101
    strt = time.process_time()
    for cidx, corr_label in enumerate(curr_run):
        max_func = partial(subtract_max, corr=cidx)
        with Pool(NUM_WORKERS) as p:  # pylint: disable=E1129
            tol_and_lbl = p.map(max_func, corr_label[NUM_POINTS:])
        tol_and_lbl = np.asarray(tol_and_lbl)
        tolerance_to_any.append(tol_and_lbl[:, :2])
        prediction_rates.append(tol_and_lbl[:, 2])
    logging.info('%.4fS to compute', time.process_time() - strt)

    assert(len(tolerance_to_any) != 0), 'Tolerances are the wrong length'
    assert(len(prediction_rates) != 0), 'Predictions are the wrong length'

    return average_at_evals(tolerance_to_any, pred_rate=prediction_rates)


def plot_eval(runInfo):
    """Plot evaluation accuracy over time for each run in the configuration"""
    accuracy_fig = plt.figure()
    accuracy_axs = accuracy_fig.add_subplot(1, 1, 1)
    accuracy_axs.set_title(runInfo.format_name('Accuracy on Validation set'))
    accuracy_axs.set_xlabel('Time (Seconds since start of training)')
    accuracy_axs.set_ylabel('Top-1 Accuracy')
    accuracy_axs.legend(loc='lower right')

    for run, d in enumerate(runInfo.load_all_eval()):
        nd = np.asarray(d)
        accuracy_axs.plot(nd[:, 0], d[:, 1], label="Run {}".format(run))

        accuracy_fig.savefig(runInfo.format_name() + '_' + run + '_eval.png')


def plot_confidences(runInfo, targ_axs=None, indsc_axs=None):
    """Plot targeted and indiscriminate tolerance for each run in the
    configuration

    Can directly modify axis instead of creating new ones, eg, for generating a
    page of graphs"""
    # prediction rate statistics:
    # ten graphs, one for each correct label:
    # each graph has ten lines, one for each label:
    # at each point in time, how many samples/1000 were predicted to belong
    # to the current label
    pred_rate_fig = plt.figure(figsize=(30, 20))
    # for each image
    #   predicted_value = max(confidences)

    for ridx, run in enumerate(runInfo.load_all_preds()):
        logging.info('Processing %i', ridx)
        # targeted tolerances figure; only if this run had a target label
        if runInfo.target is not None:
            if targ_axs is None:
                targ_tol_fig = plt.figure()
                targ_tol_axs = targ_tol_fig.add_subplot(1, 1, 1)
            else:
                targ_tol_axs = targ_axs
            targ_tol_axs.set_title(runInfo.format_name(
                'Targeted to {}'.format(runInfo.target)))
            targ_tol_axs.set_xlabel('Time (Seconds since start of training)')
            targ_tol_axs.set_ylabel('Tolerance towards label {}'.format(
                runInfo.target))
            targ_tol_axs.legend(loc='lower right')

        # indiscriminate tolerances figure
        if indsc_axs is None:
            indsc_tol_fig = plt.figure()
            indsc_tol_axs = indsc_tol_fig.add_subplot(1, 1, 1)
        else:
            indsc_tol_axs = indsc_axs
        indsc_tol_axs.set_title(runInfo.format_name('Indiscriminate'))
        indsc_tol_axs.set_xlabel('Time (Seconds since start of training)')
        indsc_tol_axs.set_ylabel('Tolerance towards next highest')
        indsc_tol_axs.legend(loc='lower right')

        # actually calculate the tolerances and prediction rates
        indsc_tolerance, pred_rate = compute_indiscriminate(run)

        if runInfo.target is not None:
            targ_tolerance = compute_targeted(run, runInfo)
            itera = zip(targ_tolerance, indsc_tolerance, pred_rate)
        else:
            # count is a really ugly solution here, but it does the job.
            # Really, targ_tolerance doesn't exist when not running with a
            # target, but we still want to iterate over the following loop,
            # this is just a silly way to avoid having to change the logic :(
            itera = zip(count(), indsc_tolerance, pred_rate)

        # for each correct label, plot!
        # tolerances: generates a single line for each class
        # prediciton rates: generate a single sub-graph with ten rates for each
        # class
        for lbl, (tt, it, pr) in enumerate(itera):
            # prediction rates subgraph in the prediction rates figure
            pred_rate_axs = pred_rate_fig.add_subplot(2, 5, lbl+1)
            pred_rate_axs.set_title('Label {}'.format(lbl))
            pred_rate_axs.set_xlabel('Time')
            pred_rate_axs.set_ylabel('Prediction rate')

            nt = np.asarray(it)
            nt_extended = np.zeros((len(nt)+1, 2))
            nt_extended[:-1] = nt
            nt_extended[-1] = nt[-1]
            nt_extended[-1, 0] += 10
            indsc_tol_axs.plot(nt_extended[:, 0],
                               nt_extended[:, 1],
                               label='Label {}'.format(lbl))

            if runInfo.target is not None:
                nt = np.asarray(tt)
                nt_extended[:-1, 1] = nt[:, 1]
                nt_extended[-1, 1] = nt[-1, 1]
                targ_tol_axs.plot(nt_extended[:, 0],
                                  nt_extended[:, 1],
                                  label='Label {}'.format(lbl))

            nt = np.stack(pr)
            for currLabel in range(10):
                # the timing information is already set, since it matches that
                # of the tolerances; make life simple and just leave it
                # untouched
                nt_extended[:-1, 1] = nt[:, currLabel]
                nt_extended[-1, 1] = nt[-1, currLabel]

                pred_rate_axs.plot(nt_extended[:, 0],
                                   nt_extended[:, 1],
                                   label='Label {}'.format(currLabel))
            pred_rate_axs.legend(loc='lower left')

        # TODO change destination path
        if runInfo.target is not None:
            targ_tol_fig.savefig(runInfo.format_name() +
                                 '_{}_targ.png'.format(ridx))

        indsc_tol_fig.savefig(runInfo.format_name() +
                              '_{}_indsc.png'.format(ridx))

        pred_rate_fig.savefig(runInfo.format_name() +
                              '_{}_predR.png'.format(ridx))


if __name__ == '__main__':
    FORMAT = '%(message)s [%(levelno)s-%(asctime)s %(module)s:%(funcName)s]'
    logging.basicConfig(level=logging.DEBUG, format=FORMAT)
    parser.add_argument('filepath', type=str)
    args = parser.parse_args()

    run_info = hogwild_run(args.filepath)

    plot_eval(run_info)
    plot_confidences(run_info)

    logging.info('Finished plotting')
